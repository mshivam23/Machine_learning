{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mshivam23/Machine_learning/blob/main/Online%20writer%20identification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88897rzswQz1",
        "outputId": "6b3b6ea9-18e6-43fa-ffc0-6a26200df915"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm==4.36.1 in /usr/local/lib/python3.7/dist-packages (4.36.1)\n",
            "Requirement already satisfied: tensorflow_gpu==2.0.0 in /usr/local/lib/python3.7/dist-packages (2.0.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==2.0.0) (1.0.8)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==2.0.0) (1.15.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==2.0.0) (0.2.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==2.0.0) (1.44.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==2.0.0) (0.8.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==2.0.0) (3.3.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==2.0.0) (0.2.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==2.0.0) (0.37.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==2.0.0) (3.17.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==2.0.0) (1.14.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==2.0.0) (1.17.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==2.0.0) (1.1.2)\n",
            "Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==2.0.0) (2.0.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==2.0.0) (1.0.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==2.0.0) (2.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow_gpu==2.0.0) (3.1.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow_gpu==2.0.0) (57.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow_gpu==2.0.0) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow_gpu==2.0.0) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow_gpu==2.0.0) (3.3.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow_gpu==2.0.0) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow_gpu==2.0.0) (1.35.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow_gpu==2.0.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow_gpu==2.0.0) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow_gpu==2.0.0) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow_gpu==2.0.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow_gpu==2.0.0) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow_gpu==2.0.0) (4.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow_gpu==2.0.0) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow_gpu==2.0.0) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow_gpu==2.0.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow_gpu==2.0.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow_gpu==2.0.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow_gpu==2.0.0) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow_gpu==2.0.0) (3.2.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow_gpu==2.0.0) (1.5.2)\n",
            "Requirement already satisfied: tensorflow==2.0.0 in /usr/local/lib/python3.7/dist-packages (2.0.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.0.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (0.8.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.44.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.14.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.17.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (2.0.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (3.3.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.0.8)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (3.17.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.1.2)\n",
            "Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (2.0.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (0.37.1)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (0.2.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0.0) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.3.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (57.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.6)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.35.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.2.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==2.0.0) (1.5.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tqdm==4.36.1\n",
        "!pip install tensorflow_gpu==2.0.0\n",
        "!pip install tensorflow==2.0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMPyyCqixY1e",
        "outputId": "f6b2d932-e9a5-4bd5-af0b-4e034ecbdb13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.17.3 in /usr/local/lib/python3.7/dist-packages (1.17.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy==1.17.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lTU3wAcpxbz2"
      },
      "outputs": [],
      "source": [
        "#importing models and files\n",
        "import json\n",
        "from random import choice, sample, randint\n",
        "import numpy as np\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.layers import Input, Dense, GlobalMaxPool1D, GlobalAvgPool1D, Concatenate, Multiply, Dropout, \\\n",
        "    Subtract, Bidirectional, MaxPool1D, GRU\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.python.ops import math_ops\n",
        "import tensorflow\n",
        "tensorflow.keras.metrics.SparseCategoricalAccuracy(name='sparse_categorical_accuracy', dtype=None)\n",
        "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import AvgPool1D\n",
        "from sklearn import preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rIsNUkyZxdlP"
      },
      "outputs": [],
      "source": [
        "#for limiting the stoke value length to max size=40, coz strokes are not of equal length\n",
        "def cap(seq, cap_val=0, max_size=500):   #will fetch the stroke sequence \n",
        "    seq = seq[:max_size]                    #will take the sequence till the max size\n",
        "    seq = seq + [cap_val] * (max_size - len(seq))   #if the length of the stroke is less than 40, it will add zeros in order to limit the lenght to 40\n",
        "    return seq                           #returning the sequenece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FMKg5jDuxf7s"
      },
      "outputs": [],
      "source": [
        "#we would prefer to go for relative features so that model will be insensitive to the absolue position,but it will take the relative positions, so it will only be sensitive to the gap between two samples\n",
        "def to_delta(x):                                      # for calculating the relative features, \n",
        "    x = [0] + x\n",
        "    deltas = [b - a for a, b in zip(x[:-1], x[1:])]\n",
        "    return deltas            #returning the relative features\n",
        "\n",
        "def to_delta2(x):                                      # for calculating the relative features, \n",
        "    x = [0] + x\n",
        "    deltas = [b - a for a, b in zip(x[:-1], x[20:])]\n",
        "    return deltas            #returning the relative features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SooyNI1AxksX"
      },
      "outputs": [],
      "source": [
        "def normalize(arr):\n",
        "    norm_arr = []\n",
        "    diff = 1\n",
        "    diff_arr = max(arr) - min(arr)    \n",
        "    for i in arr:\n",
        "        temp = (((i - min(arr))*diff)/diff_arr) \n",
        "        norm_arr.append(temp)\n",
        "    return norm_arr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "HIFw4MpmxlIu"
      },
      "outputs": [],
      "source": [
        "#for generating sample or substrokes\n",
        "def generate_sample_from_strokes(data, size_max=1300, pen_val=10):\n",
        "    x = []           #X parameters\n",
        "    y = []           #Y parameters\n",
        "    pen = []         #pen up and down information\n",
        "    for stroke in data['strokes']:               #loop for generating the samples to be fedded \n",
        "        x += stroke['x']                    #will keep on adding the x \n",
        "        y += stroke['y']                     #will keep on adding the Y\n",
        "        _pen = [0] * len(stroke['x'])         \n",
        "        _pen[0] = pen_val\n",
        "        pen += _pen                       #will keep on adding pen up and down information\n",
        "\n",
        "    pen = cap(pen, cap_val=0, max_size=size_max)   #cap function will limit the pen info to be limited to max_size=40\n",
        "    x = cap(x, cap_val=0, max_size=size_max)       #will give the X sub-stroke limited to length=40\n",
        "    y = cap(y, cap_val=0, max_size=size_max)        # will give the substroke of Y limited to max length=40\n",
        "    x1 = to_delta(x)                                 #will give the relative position of X\n",
        "    y1 = to_delta(y)\n",
        "\n",
        "\n",
        "\n",
        "    return list(zip(x1,y1,pen))                     # will give the stoke having X,Y,pen info as features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4l_Go_fQxqF6"
      },
      "outputs": [],
      "source": [
        "#for generating sample or substrokes\n",
        "def generate_sample_from_strokes(data, size_max=1000, pen_val=10):\n",
        "    x = []           #X parameters\n",
        "    y = []           #Y parameters\n",
        "    pen = []         #pen up and down information\n",
        "    for stroke in data['strokes']:               #loop for generating the samples to be fedded \n",
        "        x += stroke['x']                    #will keep on adding the x \n",
        "        y += stroke['y']                     #will keep on adding the Y\n",
        "        _pen = [0] * len(stroke['x'])         \n",
        "        _pen[0] = pen_val\n",
        "        pen += _pen                       #will keep on adding pen up and down information\n",
        "\n",
        "    pen = cap(pen, cap_val=0, max_size=size_max)   #cap function will limit the pen info to be limited to max_size=40\n",
        "    x = cap(x, cap_val=0, max_size=size_max)       #will give the X sub-stroke limited to length=40\n",
        "    y = cap(y, cap_val=0, max_size=size_max)        # will give the substroke of Y limited to max length=40\n",
        "    x1 = to_delta(x)                                 #will give the relative position of X\n",
        "    y1 = to_delta(y)\n",
        "    x2= to_delta2(x)\n",
        "    y2=to_delta2(y)\n",
        "\n",
        "\n",
        "\n",
        "    return list(zip(x1,y1,x2,y2,pen))                     # will give the stoke having X,Y,pen info as features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "UoJ4AvP2xq-P"
      },
      "outputs": [],
      "source": [
        "def read_sample(path):                #for reding the data of strokes from files from json files\n",
        "    with open(path, \"r\") as f:            \n",
        "        data = json.load(f)                                  #loading data from json files\n",
        "    return generate_sample_from_strokes(data)                #generating samples from the strokes of the data\n",
        "\n",
        "from itertools import islice\n",
        "def take(n, iterable):\n",
        "    \"Return first n items of the iterable as a list\"\n",
        "    return list(islice(iterable, n))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-VU9ROJdxsnH"
      },
      "outputs": [],
      "source": [
        "def gen(writers_map, batch_size=128):\n",
        "  writers = list(writers_map.keys())\n",
        "  while True:\n",
        "    labels = [randint(0, 49) for _ in range(128)]\n",
        "\n",
        "    X = [choice(writers_map[writers[index]]) for index in labels]\n",
        "    X = np.array([read_sample(x) for x in X])/10\n",
        "    yield X, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "WIJK92RexvDi"
      },
      "outputs": [],
      "source": [
        "def encoder(seq_dim=3):\n",
        "    in_seq_input = Input((None, seq_dim), name='in_sequence')\n",
        "    in_seq = Bidirectional(LSTM(10, return_sequences=True), name=\"gru_1\")(in_seq_input)\n",
        "    in_seq = MaxPool1D(pool_size=10)(in_seq)\n",
        "    in_seq = Bidirectional(LSTM(25, return_sequences=True), name=\"gru_2\")(in_seq_input)\n",
        "    in_seq = MaxPool1D(pool_size=10)(in_seq)\n",
        "    in_seq = Bidirectional(LSTM(50, return_sequences=True), name=\"gru_3\")(in_seq_input)\n",
        "    model = Model(in_seq_input, in_seq)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "w0XFDLL7yJb6"
      },
      "outputs": [],
      "source": [
        "def baseline_model(seq_dim=3):\n",
        "    input_1 = Input(shape=(None, 3))     #defining input for the model\n",
        "    base_model = encoder(seq_dim=3)    # for passing the input from the model\n",
        "    x1 = base_model(input_1)      #getting first input as delta X  \n",
        "    x1 = Concatenate(axis=-1)([GlobalMaxPool1D()(x1), GlobalAvgPool1D()(x1)])     \n",
        "    x = Dense(1024, activation=\"relu\")(x1)# can put any at 100 place, can try with other number as well\n",
        "    x = Dropout(0.5)(x)\n",
        "    out =Dense(50, activation=\"softmax\")(x)\n",
        "    model = Model([input_1], out)\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\", metrics=['SparseCategoricalAccuracy'], optimizer=Adam(0.001))\n",
        "    model.summary()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JyivxQCyRE1",
        "outputId": "a041999b-d770-47dd-b8f7-6e60f7d6ced1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3EYAT9cyTC1",
        "outputId": "967873fa-8570-44ab-9094-7859e869d41f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, None, 3)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "model_2 (Model)                 (None, None, 100)    21600       input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_1 (GlobalM (None, 100)          0           model_2[1][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 100)          0           model_2[1][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 200)          0           global_max_pooling1d_1[0][0]     \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1024)         205824      concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 1024)         0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 50)           51250       dropout_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 278,674\n",
            "Trainable params: 278,674\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "No model to load\n",
            "WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "Epoch 1/50\n",
            "199/200 [============================>.] - ETA: 7s - loss: 3.0422 - SparseCategoricalAccuracy: 0.2057 WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00001: val_SparseCategoricalAccuracy improved from -inf to 0.47195, saving model to /content/gdrive/MyDrive/project/handwriting_online_LSTM5.h5\n",
            "200/200 [==============================] - 1789s 9s/step - loss: 3.0369 - SparseCategoricalAccuracy: 0.2069 - val_loss: 1.9351 - val_SparseCategoricalAccuracy: 0.4720\n",
            "Epoch 2/50\n",
            "199/200 [============================>.] - ETA: 7s - loss: 1.4529 - SparseCategoricalAccuracy: 0.5806 WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00002: val_SparseCategoricalAccuracy improved from 0.47195 to 0.68086, saving model to /content/gdrive/MyDrive/project/handwriting_online_LSTM5.h5\n",
            "200/200 [==============================] - 1731s 9s/step - loss: 1.4514 - SparseCategoricalAccuracy: 0.5810 - val_loss: 1.2010 - val_SparseCategoricalAccuracy: 0.6809\n",
            "Epoch 3/50\n",
            "199/200 [============================>.] - ETA: 7s - loss: 0.8822 - SparseCategoricalAccuracy: 0.7464 WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00003: val_SparseCategoricalAccuracy improved from 0.68086 to 0.73133, saving model to /content/gdrive/MyDrive/project/handwriting_online_LSTM5.h5\n",
            "200/200 [==============================] - 1736s 9s/step - loss: 0.8813 - SparseCategoricalAccuracy: 0.7466 - val_loss: 0.8640 - val_SparseCategoricalAccuracy: 0.7313\n",
            "Epoch 4/50\n",
            " 19/200 [=>............................] - ETA: 21:56 - loss: 0.7047 - SparseCategoricalAccuracy: 0.7911"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    import random\n",
        "    from itertools import islice\n",
        "\n",
        "    with open('/content/gdrive/MyDrive/project/p/writer_json_mapping.json', 'r') as f:\n",
        "        writer_json_mapping = json.load(f) # loading json file (contains mapping writert to json strokes files)\n",
        "    indexes=take(50,writer_json_mapping)\n",
        "    Dict={}\n",
        "    t=0\n",
        "    for i in indexes:\n",
        "      if i in writer_json_mapping.keys():\n",
        "        Dict[t]=writer_json_mapping[i]\n",
        "        t=t+1\n",
        "    \n",
        "\n",
        "    train_writers_mapping = {}\n",
        "    val_writers_mapping = {}\n",
        "\n",
        "    for k, v in Dict.items():\n",
        "      train = []\n",
        "      val = []\n",
        "\n",
        "      for writing in v:\n",
        "          if random.random() < 0.75 or not train:\n",
        "              train.append(writing)\n",
        "          else:\n",
        "              val.append(writing)\n",
        "\n",
        "      train_writers_mapping[k] = train\n",
        "      val_writers_mapping[k] = val\n",
        "      if not val:\n",
        "          val_writers_mapping[k] = train\n",
        "\n",
        "    file_path = \"/content/gdrive/MyDrive/project/handwriting_online_LSTM5.h5\"\n",
        "\n",
        "    checkpoint = ModelCheckpoint(file_path, monitor='val_SparseCategoricalAccuracy', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "    reduce_on_plateau = ReduceLROnPlateau(monitor=\"val_SparseCategoricalAccuracy\", mode=\"max\", factor=0.01, patience=2, verbose=1)\n",
        "\n",
        "    callbacks_list = [checkpoint, reduce_on_plateau]\n",
        "\n",
        "    model = baseline_model()\n",
        "\n",
        "    try:\n",
        "        model.load_weights(file_path)\n",
        "    except:\n",
        "        print(\"No model to load\")\n",
        "\n",
        "    hist=model.fit_generator(gen(train_writers_mapping, batch_size=32), use_multiprocessing=True,\n",
        "                        validation_data=gen(val_writers_mapping, batch_size=32), epochs=50, verbose=1,\n",
        "                        workers=4, callbacks=callbacks_list, steps_per_epoch=200, validation_steps=100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "faSYkh2BybpI",
        "outputId": "3280dd5d-2d56-4327-de60-ec41c28a1d5e"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-26ca93e280d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_writers_mapping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "eval = model.evaluate_generator(gen(val_writers_mapping, batch_size=32), steps=100)\n",
        "eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiIwyM9ecx7K"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "train_loss=hist.history['loss']\n",
        "val_loss=hist.history['val_loss']\n",
        "train_acc=hist.history['SparseCategoricalAccuracy']\n",
        "val_acc=hist.history['val_SparseCategoricalAccuracy']\n",
        "xc=range(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGAnUq7Kc2Yh"
      },
      "outputs": [],
      "source": [
        "plt.figure(1,figsize=(7,5))\n",
        "plt.plot(xc,train_loss)\n",
        "plt.plot(xc,val_loss)\n",
        "plt.xlabel('num of Epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.title('train_loss vs val_loss')\n",
        "plt.grid(True)\n",
        "plt.legend(['train','val'])\n",
        "print (plt.style.available) # use bmh, classic,ggplot for big pictures\n",
        "plt.style.use(['classic'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSN2fYSxc5XZ"
      },
      "outputs": [],
      "source": [
        "plt.figure(2,figsize=(7,5))\n",
        "plt.plot(xc,train_acc)\n",
        "plt.plot(xc,val_acc)\n",
        "plt.xlabel('num of Epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.title('train_acc vs val_acc')\n",
        "plt.grid(True)\n",
        "plt.legend(['train','val'],loc=4)\n",
        "#print plt.style.available # use bmh, classic,ggplot for big pictures\n",
        "plt.style.use(['classic'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3leHhuoWc3nS"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "LSTM.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPPApd5fbGTr0h9JlMszME9",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}